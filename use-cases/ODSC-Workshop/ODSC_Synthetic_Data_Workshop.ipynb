{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸš€ Build Synthetic Datasets with Cerebras + Synthetic Data Kit\n",
        "\n",
        "Checkout: Synthetic-Data-Kit here: https://github.com/meta-llama/synthetic-data-kit/\n",
        "\n",
        "**ODSC Workshop - From Research Paper to Fine-Tuning Dataset**\n",
        "\n",
        "In this notebook, you'll:\n",
        "- âœ… Parse the Llama 3 research paper\n",
        "- âœ… Generate 50+ Q&A pairs using Cerebras inference\n",
        "- âœ… Filter for quality using LLM-as-judge\n",
        "- âœ… Export to fine-tuning format\n",
        "\n",
        "**No coding required - just run the cells!** âš¡"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”‘ Step 1: Set Your Cerebras API Key\n",
        "\n",
        "Enter your Cerebras API key below:"
      ],
      "metadata": {
        "id": "api_key_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Enter your API key directly (not recommended for sharing)\n",
        "CEREBRAS_API_KEY = \"csk-jn2394dxhp5vmv4w36x4k93t36eryc9cdr9rxnm3fdffkwk8\"\n",
        "\n",
        "# Option 2: Use Colab Secrets (recommended - add key as 'CEREBRAS_API_KEY' in secrets)\n",
        "# Uncomment below if using secrets:\n",
        "# CEREBRAS_API_KEY = userdata.get('CEREBRAS_API_KEY')\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['CEREBRAS_API_KEY'] = CEREBRAS_API_KEY\n",
        "\n",
        "print(\"âœ… API key configured!\")\n",
        "print(f\"ğŸ”‘ Key preview: {CEREBRAS_API_KEY[:10]}...\")"
      ],
      "metadata": {
        "id": "api_key_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f732fa2-5fdf-4de7-b935-45b6e768322d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API key configured!\n",
            "ğŸ”‘ Key preview: csk-kktf8h...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“¦ Step 2: Install Synthetic Data Kit\n",
        "\n",
        "Installing the toolkit and dependencies..."
      ],
      "metadata": {
        "id": "install_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q synthetic-data-kit\n",
        "!pip install -q datasets  # For HuggingFace format export\n",
        "\n",
        "# Verify installation\n",
        "!synthetic-data-kit --help | head -15\n",
        "\n",
        "print(\"\\nâœ… Installation complete!\")"
      ],
      "metadata": {
        "id": "install_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3781f207-fc80-4eba-8b9d-22f3c9e4a4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/79.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.5/152.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "                                                                                \n",
            " Usage: synthetic-data-kit [OPTIONS] COMMAND [ARGS]...                          \n",
            "                                                                                \n",
            " A toolkit for preparing synthetic datasets for fine-tuning LLMs                \n",
            "                                                                                \n",
            "â•­â”€ Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
            "â”‚ --config              -c      PATH  Path to configuration file               â”‚\n",
            "â”‚ --install-completion                Install completion for the current       â”‚\n",
            "â”‚                                     shell.                                   â”‚\n",
            "â”‚ --show-completion                   Show completion for the current shell,   â”‚\n",
            "â”‚                                     to copy it or customize the              â”‚\n",
            "\n",
            "âœ… Installation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Step 3: Download Workshop Configuration\n",
        "\n",
        "Downloading the ready-to-use config from GitHub and setting up directories..."
      ],
      "metadata": {
        "id": "setup_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory structure\n",
        "!mkdir -p data/{parsed,generated,curated,final}\n",
        "\n",
        "print(\"ğŸ“¥ Downloading workshop config from GitHub...\")\n",
        "\n",
        "# Download the ready-to-use config from GitHub (ODSC-Workshop branch)\n",
        "!wget -q https://raw.githubusercontent.com/meta-llama/synthetic-data-kit/ODSC-Workshop/configs/config.yaml -O cerebras_config.yaml\n",
        "\n",
        "print(\"âœ… Config downloaded!\")\n",
        "\n",
        "# Replace the API key placeholder with your actual key\n",
        "import os\n",
        "\n",
        "with open('cerebras_config.yaml', 'r') as f:\n",
        "    config_content = f.read()\n",
        "\n",
        "# Replace the placeholder with actual API key\n",
        "config_content = config_content.replace('YOUR_CEREBRAS_API_KEY', os.environ.get('CEREBRAS_API_KEY'))\n",
        "\n",
        "with open('cerebras_config.yaml', 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"âœ… Configuration ready with your API key!\")\n",
        "print(\"\\nğŸ“ Directory structure:\")\n",
        "!tree data/ || ls -R data/\n",
        "\n",
        "print(\"\\nğŸ“„ Config preview (first 35 lines):\")\n",
        "!head -35 cerebras_config.yaml"
      ],
      "metadata": {
        "id": "setup_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fafbc2-5d50-4268-b78c-6049acf3e2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Downloading workshop config from GitHub...\n",
            "âœ… Config downloaded!\n",
            "âœ… Configuration ready with your API key!\n",
            "\n",
            "ğŸ“ Directory structure:\n",
            "/bin/bash: line 1: tree: command not found\n",
            "data/:\n",
            "curated  final\tgenerated  input  parsed\n",
            "\n",
            "data/curated:\n",
            "\n",
            "data/final:\n",
            "\n",
            "data/generated:\n",
            "\n",
            "data/input:\n",
            "\n",
            "data/parsed:\n",
            "\n",
            "ğŸ“„ Config preview (first 35 lines):\n",
            "# Master configuration file for Synthetic Data Kit\n",
            "# Workshop-ready configuration with Cerebras defaults\n",
            "\n",
            "# Global paths configuration\n",
            "paths:\n",
            "  # Input data location (directory containing files to process)\n",
            "  input: \"data/input\"           # Directory containing PDF, HTML, DOCX, PPT, TXT files\n",
            "\n",
            "  # Output locations (4-stage pipeline directories)\n",
            "  output:\n",
            "    parsed: \"data/parsed\"       # Stage 1: Where parsed text files are saved (ingest output)\n",
            "    generated: \"data/generated\" # Stage 2: Where generated QA pairs are saved (create output)\n",
            "    curated: \"data/curated\"     # Stage 3: Where curated QA pairs are saved (curate output)\n",
            "    final: \"data/final\"         # Stage 4: Where final training formats are saved (save-as output)\n",
            "\n",
            "# LLM Provider configuration\n",
            "llm:\n",
            "  # Provider selection: \"vllm\" or \"api-endpoint\"\n",
            "  provider: \"api-endpoint\"\n",
            "\n",
            "# VLLM server configuration\n",
            "vllm:\n",
            "  api_base: \"http://localhost:8000/v1\" # Base URL for VLLM API\n",
            "  port: 8000                           # Port for VLLM server\n",
            "  model: \"meta-llama/Llama-3.3-70B-Instruct\" # Default model to use\n",
            "  max_retries: 3                       # Number of retries for API calls\n",
            "  retry_delay: 1.0                     # Initial delay between retries (seconds)\n",
            "  sleep_time: 0.1                      # Small delay in seconds between batches to avoid rate limits\n",
            "\n",
            "# API endpoint configuration (Cerebras defaults)\n",
            "api-endpoint:\n",
            "  api_base: \"https://api.cerebras.ai/v1\" # Cerebras API endpoint\n",
            "  api_key: \"csk-kktf8hhne6hcc94c42f3vvrnpp9wecm226t4kxjkvxfhm9nd\"        # Replace with your Cerebras API key\n",
            "  model: \"llama3.3-70b\"                   # Cerebras Llama 3.3 70B model\n",
            "  max_retries: 3                          # Number of retries for API calls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”Œ Step 4: Test API Connection\n",
        "\n",
        "Verifying connection to Cerebras..."
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit -c cerebras_config.yaml system-check\n",
        "\n",
        "print(\"\\nâœ… If you see 'API endpoint access confirmed' above, you're ready to go!\")"
      ],
      "metadata": {
        "id": "test_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88910a3-d212-4c08-af60-723498a81376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[1;34mEnvironment variable check:\u001b[0m\n",
            "API_ENDPOINT_KEY: Not found\n",
            "get_llm_provider returning: api-endpoint\n",
            "API_ENDPOINT_KEY environment variable: Not found\n",
            "API key source: Config file\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Checking API endpoint access...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m API endpoint access confirmed\u001b[0m\n",
            "\u001b[2K\u001b[32mUsing custom API base: \u001b[0m\u001b[4;94mhttps://api.cerebras.ai/v1\u001b[0m\n",
            "\u001b[2K\u001b[32mDefault model: llama3.\u001b[0m\u001b[1;36m3\u001b[0m\u001b[32m-70b\u001b[0m\n",
            "\u001b[2K\u001b[32mResponse from model: Hello. How can I assist you today?\u001b[0m\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Checking API endpoint access...\n",
            "\u001b[1A\u001b[2K\n",
            "âœ… If you see 'API endpoint access confirmed' above, you're ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“¥ Step 5: Download Llama 3 Paper\n",
        "\n",
        "Downloading the research paper from arXiv..."
      ],
      "metadata": {
        "id": "download_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://arxiv.org/pdf/2407.21783 -O llama3_paper.pdf\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "file_size = os.path.getsize('llama3_paper.pdf') / 1024  # KB\n",
        "\n",
        "print(f\"âœ… Paper downloaded successfully!\")\n",
        "print(f\"ğŸ“„ File: llama3_paper.pdf\")\n",
        "print(f\"ğŸ’¾ Size: {file_size:.1f} KB\")\n",
        "\n",
        "!ls -lh llama3_paper.pdf"
      ],
      "metadata": {
        "id": "download_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1621adf5-d8a8-46dc-f733-dedd47ee4e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Paper downloaded successfully!\n",
            "ğŸ“„ File: llama3_paper.pdf\n",
            "ğŸ’¾ Size: 9602.7 KB\n",
            "-rw-r--r-- 1 root root 9.4M Nov 26  2024 llama3_paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ”„ The 4-Stage Pipeline\n",
        "\n",
        "```\n",
        "PDF â†’ INGEST â†’ CREATE â†’ CURATE â†’ SAVE-AS â†’ Training Data âœ¨\n",
        "```"
      ],
      "metadata": {
        "id": "pipeline_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š Stage 1: INGEST - Parse the PDF\n",
        "\n",
        "**What it does:** Extracts clean text from the PDF and saves as .txt\n",
        "\n",
        "This takes ~30-60 seconds..."
      ],
      "metadata": {
        "id": "ingest_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  ingest llama3_paper.pdf\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… INGEST complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check output\n",
        "!ls -lh data/parsed/\n",
        "\n",
        "# Preview first few lines of the extracted text\n",
        "print(\"\\nğŸ“ Preview of extracted text:\")\n",
        "!head -20 data/parsed/llama3_paper.txt"
      ],
      "metadata": {
        "id": "ingest_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e224df8-4f75-46ad-dc55-a6dcd4d30bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Processing llama3_paper.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32mâœ… Text successfully extracted to \u001b[0m\u001b[1;32mdata/parsed/llama3_paper.txt\u001b[0m\n",
            "\n",
            "============================================================\n",
            "âœ… INGEST complete!\n",
            "============================================================\n",
            "total 352K\n",
            "-rw-r--r-- 1 root root 352K Oct 28 14:51 llama3_paper.txt\n",
            "\n",
            "ğŸ“ Preview of extracted text:\n",
            "4\n",
            "2\n",
            "0\n",
            "2\n",
            "\n",
            "v\n",
            "o\n",
            "N\n",
            "3\n",
            "2\n",
            "\n",
            "]\n",
            "I\n",
            "\n",
            "A\n",
            ".\n",
            "s\n",
            "c\n",
            "[\n",
            "\n",
            "CPU times: user 91 ms, sys: 6.18 ms, total: 97.2 ms\n",
            "Wall time: 15.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¤– Stage 2: CREATE - Generate Q&A Pairs\n",
        "\n",
        "**What it does:** Uses Cerebras + Llama 3.3-70B with custom prompts to generate intelligent Q&A pairs\n",
        "\n",
        "This takes ~2-4 minutes for 50 pairs... â˜•"
      ],
      "metadata": {
        "id": "create_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 50 \\\n",
        "  --verbose\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… CREATE complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check output\n",
        "!ls -lh data/generated/\n",
        "\n",
        "# Count Q&A pairs\n",
        "import json\n",
        "with open('data/generated/llama3_paper_qa_pairs.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"\\nğŸ“Š Generated {len(data['qa_pairs'])} Q&A pairs\")"
      ],
      "metadata": {
        "id": "create_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a2a330-3ba0-4421-d2a7-69cc370cdffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32mğŸ”— Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\r\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\r\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\r\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KL Using api-endpoint provider\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KGenerating document summary...\n",
            "\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Sending request to api-endpoint model llama3.3-70b...\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KSummary generated (1075 chars)\n",
            "\u001b[2KGenerating QA pairs...\n",
            "\u001b[2KDocument split into 113 chunks\n",
            "\u001b[2KUsing batch size of 5\n",
            "\u001b[2KProcessing 113 chunks to generate QA pairs...\n",
            "\u001b[2KProcessing batch 1/23 with 5 chunks\n",
            "\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 132\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 1 (total: 1/50)\n",
            "\u001b[2KParsing response of length 101\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 2 (total: 2/50)\n",
            "\u001b[2KParsing response of length 117\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 3 (total: 3/50)\n",
            "\u001b[2KParsing response of length 151\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 4 (total: 4/50)\n",
            "\u001b[2KParsing response of length 319\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 5 (total: 5/50)\n",
            "\u001b[2KProcessing batch 2/23 with 5 chunks\n",
            "\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 6 (total: 6/50)\n",
            "\u001b[2KParsing response of length 144\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 7 (total: 7/50)\n",
            "\u001b[2KParsing response of length 110\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 8 (total: 8/50)\n",
            "\u001b[2KParsing response of length 149\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 9 (total: 9/50)\n",
            "\u001b[2KParsing response of length 117\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 10 (total: 10/50)\n",
            "\u001b[2KProcessing batch 3/23 with 5 chunks\n",
            "\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:11\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 142\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 11 (total: 11/50)\n",
            "\u001b[2KParsing response of length 167\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 12 (total: 12/50)\n",
            "\u001b[2KParsing response of length 156\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 13 (total: 13/50)\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 14 (total: 14/50)\n",
            "\u001b[2KParsing response of length 200\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 15 (total: 15/50)\n",
            "\u001b[2KProcessing batch 4/23 with 5 chunks\n",
            "\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:16\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:19\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 158\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 16 (total: 16/50)\n",
            "\u001b[2KParsing response of length 258\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 17 (total: 17/50)\n",
            "\u001b[2KParsing response of length 172\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 18 (total: 18/50)\n",
            "\u001b[2KParsing response of length 158\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 19 (total: 19/50)\n",
            "\u001b[2KParsing response of length 131\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 20 (total: 20/50)\n",
            "\u001b[2KProcessing batch 5/23 with 5 chunks\n",
            "\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:20\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:21\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:21\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 122\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 21 (total: 21/50)\n",
            "\u001b[2KParsing response of length 267\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 22 (total: 22/50)\n",
            "\u001b[2KParsing response of length 202\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 23 (total: 23/50)\n",
            "\u001b[2KParsing response of length 195\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 24 (total: 24/50)\n",
            "\u001b[2KParsing response of length 380\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 25 (total: 25/50)\n",
            "\u001b[2KProcessing batch 6/23 with 5 chunks\n",
            "\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:25\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 2/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:35\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:37\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 271\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 26 (total: 26/50)\n",
            "\u001b[2KParsing response of length 160\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 27 (total: 27/50)\n",
            "\u001b[2KParsing response of length 240\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 28 (total: 28/50)\n",
            "\u001b[2KParsing response of length 254\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 29 (total: 29/50)\n",
            "\u001b[2KParsing response of length 113\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 30 (total: 30/50)\n",
            "\u001b[2KProcessing batch 7/23 with 5 chunks\n",
            "\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:38\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:39\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': 'Requests per second limit exceeded - too many requests sent.', 'type': 'too_many_requests_error', 'param': 'quota', 'code': 'request_quota_exceeded'}\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:41\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:41\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 2/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:43\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 3/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:48\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 200\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 31 (total: 31/50)\n",
            "\u001b[2KParsing response of length 290\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 32 (total: 32/50)\n",
            "\u001b[2KParsing response of length 208\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 33 (total: 33/50)\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KFalling back to regex pattern matching\n",
            "\u001b[2KNo QA pairs extracted. Check the model output format.\n",
            "\u001b[2K  Generated 0 pairs from chunk 34 (total: 33/50)\n",
            "\u001b[2KParsing response of length 238\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 35 (total: 34/50)\n",
            "\u001b[2KProcessing batch 8/23 with 5 chunks\n",
            "\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:49\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 2/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:49\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 238\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 36 (total: 35/50)\n",
            "\u001b[2KParsing response of length 171\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 37 (total: 36/50)\n",
            "\u001b[2KParsing response of length 129\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 38 (total: 37/50)\n",
            "\u001b[2KParsing response of length 235\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 39 (total: 38/50)\n",
            "\u001b[2KParsing response of length 161\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 40 (total: 39/50)\n",
            "\u001b[2KProcessing batch 9/23 with 5 chunks\n",
            "\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:54\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:56\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 123\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 41 (total: 40/50)\n",
            "\u001b[2KParsing response of length 221\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 42 (total: 41/50)\n",
            "\u001b[2KParsing response of length 173\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 43 (total: 42/50)\n",
            "\u001b[2KParsing response of length 260\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 44 (total: 43/50)\n",
            "\u001b[2KParsing response of length 181\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 45 (total: 44/50)\n",
            "\u001b[2KProcessing batch 10/23 with 5 chunks\n",
            "\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:57\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 142\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 46 (total: 45/50)\n",
            "\u001b[2KParsing response of length 219\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 47 (total: 46/50)\n",
            "\u001b[2KParsing response of length 157\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 48 (total: 47/50)\n",
            "\u001b[2KParsing response of length 172\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 49 (total: 48/50)\n",
            "\u001b[2KParsing response of length 181\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 50 (total: 49/50)\n",
            "\u001b[2KProcessing batch 11/23 with 5 chunks\n",
            "\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:02\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 178\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 51 (total: 50/50)\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:02\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
            "\u001b[2KGenerated 50 QA pairs total (requested: 50)\n",
            "\u001b[2KSaving result to data/generated/llama3_paper_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/llama3_paper_qa_pairs.json\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32mâœ… Content saved to \u001b[0m\u001b[1;32mdata/generated/llama3_paper_qa_pairs.json\u001b[0m\n",
            "\n",
            "============================================================\n",
            "âœ… CREATE complete!\n",
            "============================================================\n",
            "total 16K\n",
            "-rw-r--r-- 1 root root 11K Oct 28 14:52 llama3_paper_qa_pairs.json\n",
            "-rw-r--r-- 1 root root  16 Oct 28 14:52 test_write.json\n",
            "\n",
            "ğŸ“Š Generated 50 Q&A pairs\n",
            "CPU times: user 349 ms, sys: 72.1 ms, total: 421 ms\n",
            "Wall time: 1min 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ” Preview Generated Q&A Pairs"
      ],
      "metadata": {
        "id": "preview_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load and display first 3 Q&A pairs\n",
        "with open('data/generated/llama3_paper_qa_pairs.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"ğŸ“ Summary:\")\n",
        "print(data['summary'][:200] + \"...\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“š Sample Q&A Pairs:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, pair in enumerate(data['qa_pairs'][:3], 1):\n",
        "    print(f\"\\n{i}. Question:\")\n",
        "    print(f\"   {pair['question']}\")\n",
        "    print(f\"\\n   Answer:\")\n",
        "    print(f\"   {pair['answer'][:150]}...\")\n",
        "    print(\"\\n\" + \"-\"*60)"
      ],
      "metadata": {
        "id": "preview_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e18011-4116-4c45-ad25-183cf328f46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Summary:\n",
            "Here is a summary of the document in 3-5 sentences, focusing on the main topic and key concepts:\n",
            "\n",
            "The paper introduces Llama 3, a new set of foundation models for language that natively support multil...\n",
            "\n",
            "\n",
            "============================================================\n",
            "ğŸ“š Sample Q&A Pairs:\n",
            "============================================================\n",
            "\n",
            "1. Question:\n",
            "   What is the size of the largest Llama 3 model in terms of parameters?\n",
            "\n",
            "   Answer:\n",
            "   405B parameters...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "2. Question:\n",
            "   How many parameters does the flagship model have?\n",
            "\n",
            "   Answer:\n",
            "   405B...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "3. Question:\n",
            "   What is the parameter size of the largest Llama 3 language model?\n",
            "\n",
            "   Answer:\n",
            "   405B...\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ¨ Stage 3: CURATE - Filter Quality\n",
        "\n",
        "**What it does:** Uses LLM-as-judge with custom rating prompt to rate and filter Q&A pairs\n",
        "\n",
        "This takes ~2-3 minutes... ğŸ¯"
      ],
      "metadata": {
        "id": "curate_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  curate data/generated/llama3_paper_qa_pairs.json \\\n",
        "  --threshold 7.5 \\\n",
        "  --verbose\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… CURATE complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check output\n",
        "!ls -lh data/curated/"
      ],
      "metadata": {
        "id": "curate_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c07a8d-40ec-42a4-dad0-cd0652429e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32mğŸ”— Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KProcessing 17 batches of QA pairs...\n",
            "\u001b[2KProcessing batch 1/4\n",
            "\u001b[2KSending batch request with 5 items\n",
            "\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 5 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What is the size of the largest Llama 3 model in terms of \n",
            "parameters?\", \"answer\": ...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What is the size of the context window increased to during the \n",
            "continued pre-train...\n",
            "\u001b[2KResponse 3: [\n",
            "\u001b[2K  {\"question\": \"What is the percentage of mathematical and reasoning tokens in \n",
            "the final data mix ...\n",
            "\u001b[2KResponse 4: [\n",
            "\u001b[2K  {\"question\": \"What type of GPUs were used to train the Llama 3 405B model?\", \n",
            "\"answer\": \"H100 GPU...\n",
            "\u001b[2KResponse 5: [\n",
            "\u001b[2K  {\"question\": \"How many GPUs are configured in the given example with a group \n",
            "size of |TP|=2, |CP...\n",
            "\u001b[2KProcessing batch 1\n",
            "\u001b[2KParsing ratings response of length 349\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the size of the largest Llama 3 model \n",
            "in terms of parameters?\", \"answer\": \"405B parameters\", \"rating\": 9},\\n  \n",
            "{\"question\": \"How many parameters does the flagship model have?\", \"answer\": \n",
            "\"405B\", \"rating\": 8},\\n  {\"question\": \"What is the parameter size of the largest\n",
            "Llama 3 language model?\", \"answer\": \"405B\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 128 (char 127)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 2\n",
            "\u001b[2KParsing ratings response of length 653\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the size of the context window \n",
            "increased to during the continued pre-training stage?\", \"answer\": \"128K tokens\",\n",
            "\"rating\": 8},\\n  {\"question\": \"What is the purpose of the language model \n",
            "pre-training data curation process?\", \"answer\": \"The purpose of the language \n",
            "model pre-training data curation process is to obtain high-quality tokens by \n",
            "applying several de-duplication methods and data cleaning mechanisms on each \n",
            "data source.\", \"rating\": 9},\\n  {\"question\": \"What method is'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 147 (char 146)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 3\n",
            "\u001b[2KParsing ratings response of length 422\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the percentage of mathematical and \n",
            "reasoning tokens in the final data mix of Llama 3?\", \"answer\": \"25%\", \"rating\": \n",
            "8},\\n  {\"question\": \"What is the size of the vocabulary used in Llama 3?\", \n",
            "\"answer\": \"128K tokens\", \"rating\": 9},\\n  {\"question\": \"What is the range of the\n",
            "peak learning rate used in the model training?\", \"answer\": \"Between 2 \\\\u00d7 \n",
            "10\\\\u22124 and 4 \\\\u00d7 10\\\\u22124.\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 140 (char 139)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 4\n",
            "\u001b[2KParsing ratings response of length 425\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What type of GPUs were used to train the Llama\n",
            "3 405B model?\", \"answer\": \"H100 GPUs\", \"rating\": 9},\\n  {\"question\": \"What type \n",
            "of network topology is used in the RoCE-based AI cluster?\", \"answer\": \"A \n",
            "three-layer Clos network.\", \"rating\": 9},\\n  {\"question\": \"What type of \n",
            "parallelism splits individual weight tensors into multiple chunks on different \n",
            "devices?\", \"answer\": \"Tensor parallelism.\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 113 (char 112)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 5\n",
            "\u001b[2KParsing ratings response of length 539\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"How many GPUs are configured in the given \n",
            "example with a group size of |TP|=2, |CP|=2, |PP|=2, and |DP|=2?\", \"answer\": \n",
            "\"16\", \"rating\": 8},\\n  {\"question\": \"What is the order of parallelism dimensions\n",
            "optimized for network communication?\", \"answer\": \"The order of parallelism \n",
            "dimensions is [TP, CP, PP, DP].\", \"rating\": 9},\\n  {\"question\": \"What percentage\n",
            "of unexpected interruptions were attributed to confirmed or suspected hardware \n",
            "issues during a 54-day period of Llama 3 pre-trai'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 152 (char 151)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 2/4\n",
            "\u001b[2KSending batch request with 5 items\n",
            "\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': 'Requests per minute limit exceeded - too many requests sent.', 'type': 'too_many_requests_error', 'param': 'quota', 'code': 'request_quota_exceeded'}\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:07\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 5 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What feature of PyTorch is used to diagnose hangs and \n",
            "performance issues quickly a...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What is the learning rate used for finetuning the pre-trained \n",
            "language model durin...\n",
            "\u001b[2KResponse 3: [\n",
            "\u001b[2K  {\"question\": \"What techniques are used to remove low-quality training samples \n",
            "and improve overal...\n",
            "\u001b[2KResponse 4: [\n",
            "\u001b[2K  {\"question\": \"How does the process of synthetic data generation via \n",
            "backtranslation improve cert...\n",
            "\u001b[2KResponse 5: [\n",
            "\u001b[2K  {\"question\": \"What is one way to address the discrepancy between training and \n",
            "inference in model...\n",
            "\u001b[2KProcessing batch 6\n",
            "\u001b[2KParsing ratings response of length 587\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What feature of PyTorch is used to diagnose \n",
            "hangs and performance issues quickly at scale?\", \"answer\": \"NCCL flight \n",
            "recorder\", \"rating\": 8},\\n  {\"question\": \"What happens when the power \n",
            "consumption across the data center fluctuates instantly?\", \"answer\": \"It can \n",
            "result in fluctuations of power consumption on the order of tens of megawatts, \n",
            "stretching the limits of the power grid.\", \"rating\": 9},\\n  {\"question\": \"What \n",
            "was done to the learning rate during pre-training on the final '\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 154 (char 153)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 7\n",
            "\u001b[2KParsing ratings response of length 415\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the learning rate used for finetuning \n",
            "the pre-trained language model during supervised finetuning?\", \"answer\": \n",
            "\"10\\\\u22125\", \"rating\": 8},\\n  {\"question\": \"What percentage of examples in the \n",
            "dataset are classified as General English?\", \"answer\": \"52.66%\", \"rating\": 9},\\n\n",
            "{\"question\": \"What is the average number of tokens in the context for the SFT \n",
            "data?\", \"answer\": \"656.7\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 159 (char 158)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 8\n",
            "\u001b[2KParsing ratings response of length 663\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What techniques are used to remove low-quality\n",
            "training samples and improve overall model performance?\", \"answer\": \"Model-based\n",
            "techniques such as topic classification, quality scoring, difficulty scoring, \n",
            "and semantic deduplication.\", \"rating\": 9},\\n  {\"question\": \"What is the purpose\n",
            "of training a code expert in Llama 3?\", \"answer\": \"To collect high quality human\n",
            "annotations for code throughout subsequent rounds of post-training.\", \"rating\": \n",
            "8},\\n  {\"question\": \"What percentage'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 263 (char 262)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 9\n",
            "\u001b[2KParsing ratings response of length 810\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"How does the process of synthetic data \n",
            "generation via backtranslation improve certain coding capabilities?\", \"answer\": \n",
            "\"It improves certain coding capabilities, such as documentation and \n",
            "explanations, by employing a multi-step approach where Llama 3 generates data, \n",
            "backtranslates it to the original code, and filters the output based on \n",
            "quality.\", \"rating\": 9},\\n  {\"question\": \"What was the initial result of the \n",
            "stringent filtering on downstream benchmark performance?\", \"answer\":'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 376 (char 375)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 10\n",
            "\u001b[2KParsing ratings response of length 606\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is one way to address the discrepancy \n",
            "between training and inference in models?\", \"answer\": \"Ensuring consistency \n",
            "between training and real-world usage is crucial for maintaining reasoning \n",
            "performance.\", \"rating\": 8},\\n  {\"question\": \"What is used as a feedback signal \n",
            "to ensure the correctness of the reasoning process in code execution?\", \n",
            "\"answer\": \"Code execution is used as a feedback signal to eliminate cases where \n",
            "the reasoning chain was not valid.\", \"rating\": 9},\\n  {\"qu'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 236 (char 235)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 3/4\n",
            "\u001b[2KSending batch request with 5 items\n",
            "\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:07\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:09\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:10\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 5 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What types of file formats are annotated for in the context of \n",
            "file uploads?\", \"an...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"Why do the authors not report category averages for Llama 3 \n",
            "405B?\", \"answer\": \"Bec...\n",
            "\u001b[2KResponse 3: [\n",
            "\u001b[2K  {\"question\": \"What benchmark is used for mathematical reasoning in the text?\",\n",
            "\"answer\": \"GSM8K ...\n",
            "\u001b[2KResponse 4: [\n",
            "\u001b[2K  {\"question\": \"What type of scores are reported for GRE exams in Table 17?\", \n",
            "\"answer\": \"Normalize...\n",
            "\u001b[2KResponse 5: [\n",
            "\u001b[2K  {\"question\": \"What is the performance of Llama 3 models in retrieving needles \n",
            "at all document de...\n",
            "\u001b[2KProcessing batch 11\n",
            "\u001b[2KParsing ratings response of length 702\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What types of file formats are annotated for \n",
            "in the context of file uploads?\", \"answer\": \".txt, .docx, .pdf, .pptx, .xlsx, \n",
            ".csv, .tsv, .py, .json, .jsonl, .html, .xml\", \"rating\": 9},\\n  {\"question\": \n",
            "\"What approach did the researchers take to address hallucinations in large \n",
            "language models?\", \"answer\": \"A hallucination-first approach, focusing on \n",
            "generating data that aligns model generations with subsets of factual data \n",
            "present in the pre-training data.\", \"rating\": 8},\\n  {\"questi'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 196 (char 195)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 12\n",
            "\u001b[2KParsing ratings response of length 646\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"Why do the authors not report category \n",
            "averages for Llama 3 405B?\", \"answer\": \"Because not all numbers are available \n",
            "for all benchmarks, as it is not possible to recompute benchmark values for this\n",
            "model.\", \"rating\": 8},\\n  {\"question\": \"How does Llama 3 405B perform compared \n",
            "to other models in its class?\", \"answer\": \"Llama 3 405B performs competitively \n",
            "with other models in its class and substantially outperforms prior open-source \n",
            "models.\", \"rating\": 9},\\n  {\"question\": \"What are'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 234 (char 233)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 13\n",
            "\u001b[2KParsing ratings response of length 524\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What benchmark is used for mathematical \n",
            "reasoning in the text?\", \"answer\": \"GSM8K and GSM-Plus.\", \"rating\": 8},\\n  \n",
            "{\"question\": \"How is TD selected for each dataset?\", \"answer\": \"TD is selected \n",
            "separately for each dataset, based on which value shows the maximal significant \n",
            "estimated performance gain across the three model sizes.\", \"rating\": 9},\\n  \n",
            "{\"question\": \"What benchmarks are used to evaluate Llama 3\\'s capability on \n",
            "knowledge-based question answering?\", \"answer\": \"MMLU and M'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 125 (char 124)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 14\n",
            "\u001b[2KParsing ratings response of length 521\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What type of scores are reported for GRE exams\n",
            "in Table 17?\", \"answer\": \"Normalized score\", \"rating\": 6},\\n  {\"question\": \"How \n",
            "does the performance of Llama 3 405B model compare to Claude 3.5 Sonnet and \n",
            "GPT-4o?\", \"answer\": \"The performance of Llama 3 405B model is very similar to \n",
            "Claude 3.5 Sonnet and GPT-4o.\", \"rating\": 8},\\n  {\"question\": \"How many \n",
            "languages does Llama 3 support?\", \"answer\": \"8 languages \\\\u2014 English, \n",
            "German, French, Italian, Portuguese, Hindi, Spanish, and T'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 119 (char 118)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 15\n",
            "\u001b[2KParsing ratings response of length 582\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the performance of Llama 3 models in \n",
            "retrieving needles at all document depths and context lengths?\", \"answer\": \n",
            "\"Llama 3 models demonstrate perfect needle retrieval performance, successfully \n",
            "retrieving 100% of needles.\", \"rating\": 9},\\n  {\"question\": \"What task does \n",
            "Llama 3 405B significantly beat GPT-4o in, according to Figure 16?\", \"answer\": \n",
            "\"Text-only code execution tasks and plots generation.\", \"rating\": 8},\\n  \n",
            "{\"question\": \"What scale do annotators use for their rati'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 256 (char 255)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 4/4\n",
            "\u001b[2KSending batch request with 2 items\n",
            "\u001b[32mâ ™\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 2 requests\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:10\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:10\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 2 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What is the purpose of creating internal benchmarks for model \n",
            "safety?\", \"answer\": ...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What methods were used to refine existing safety data for Llama \n",
            "3?\", \"answer\": \"A ...\n",
            "\u001b[2KProcessing batch 16\n",
            "\u001b[2KParsing ratings response of length 547\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the purpose of creating internal \n",
            "benchmarks for model safety?\", \"answer\": \"To help develop models safely and \n",
            "responsibly, and to measure the effectiveness of safety mitigations.\", \"rating\":\n",
            "9},\\n  {\"question\": \"What is the average verbatim memorization rate in \n",
            "pre-trained Llama 3 for the English, 50-gram scenario?\", \"answer\": \"1.13%\", \n",
            "\"rating\": 8},\\n  {\"question\": \"What two primary metrics are optimized for in the\n",
            "safety finetuning process?\", \"answer\": \"Violation Rate (VR'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 215 (char 214)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 17\n",
            "\u001b[2KParsing ratings response of length 359\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What methods were used to refine existing \n",
            "safety data for Llama 3?\", \"answer\": \"A combination of zero-shot rewriting and \n",
            "human-in-the-loop editing.\", \"rating\": 8},\\n  {\"question\": \"What is critical to \n",
            "consider alongside a low violation rate in evaluating a model\\'s performance?\", \n",
            "\"answer\": \"False refusal as a counter-metric.\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 177 (char 176)\n",
            "\u001b[2KSuccessfully parsed 2 items in JSON array\n",
            "\u001b[2KRating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:10\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
            "\u001b[2KRated 50 QA pairs\n",
            "\u001b[2KRetained 46 pairs (threshold: 7.5)\n",
            "\u001b[2KAverage score: 8.5\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32mâœ… Cleaned content saved to \u001b[0m\u001b[1;32mdata/curated/llama3_paper_qa_pairs_cleaned.json\u001b[0m\n",
            "\n",
            "============================================================\n",
            "âœ… CURATE complete!\n",
            "============================================================\n",
            "total 32K\n",
            "-rw-r--r-- 1 root root 30K Oct 28 14:53 llama3_paper_qa_pairs_cleaned.json\n",
            "CPU times: user 392 ms, sys: 84 ms, total: 476 ms\n",
            "Wall time: 1min 12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Š Quality Metrics"
      ],
      "metadata": {
        "id": "metrics_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load curated data\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json', 'r') as f:\n",
        "    curated = json.load(f)\n",
        "\n",
        "metrics = curated.get('metrics', {})\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š CURATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“ Total pairs generated:     {metrics.get('total', 0)}\")\n",
        "print(f\"âœ… Pairs kept (â‰¥7.5 rating):  {metrics.get('filtered', 0)}\")\n",
        "print(f\"ğŸ“ˆ Retention rate:            {metrics.get('retention_rate', 0)*100:.1f}%\")\n",
        "print(f\"â­ Average quality score:     {metrics.get('avg_score', 0):.1f}/10\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ Quality filtering complete!\")\n",
        "print(f\"   Kept {metrics.get('filtered', 0)} high-quality pairs\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "metrics_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248138f1-4c8d-467f-b7c0-583300c64010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“Š CURATION RESULTS\n",
            "============================================================\n",
            "\n",
            "ğŸ“ Total pairs generated:     50\n",
            "âœ… Pairs kept (â‰¥7.5 rating):  46\n",
            "ğŸ“ˆ Retention rate:            92.0%\n",
            "â­ Average quality score:     8.5/10\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ Quality filtering complete!\n",
            "   Kept 46 high-quality pairs\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ‘€ Preview Top-Rated Q&A Pairs"
      ],
      "metadata": {
        "id": "preview_curated_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json', 'r') as f:\n",
        "    curated = json.load(f)\n",
        "\n",
        "# Sort by rating (descending)\n",
        "sorted_pairs = sorted(curated['qa_pairs'], key=lambda x: x.get('rating', 0), reverse=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸŒŸ TOP 3 HIGHEST-RATED Q&A PAIRS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, pair in enumerate(sorted_pairs[:3], 1):\n",
        "    print(f\"\\n{i}. Rating: â­ {pair.get('rating', 'N/A')}/10\")\n",
        "    print(f\"\\n   Q: {pair['question']}\")\n",
        "    print(f\"\\n   A: {pair['answer'][:200]}...\")\n",
        "    print(\"\\n\" + \"-\"*60)"
      ],
      "metadata": {
        "id": "preview_curated_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be19cae-0204-4afb-e75c-d98c9bf6786d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸŒŸ TOP 3 HIGHEST-RATED Q&A PAIRS\n",
            "============================================================\n",
            "\n",
            "1. Rating: â­ 9/10\n",
            "\n",
            "   Q: What is the size of the largest Llama 3 model in terms of parameters?\n",
            "\n",
            "   A: 405B parameters...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "2. Rating: â­ 9/10\n",
            "\n",
            "   Q: What is the parameter size of the largest Llama 3 language model?\n",
            "\n",
            "   A: 405B...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "3. Rating: â­ 9/10\n",
            "\n",
            "   Q: What is the purpose of the language model pre-training data curation process?\n",
            "\n",
            "   A: The purpose of the language model pre-training data curation process is to obtain high-quality tokens by applying several de-duplication methods and data cleaning mechanisms on each data source....\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¾ Stage 4: SAVE-AS - Export to Training Format\n",
        "\n",
        "**What it does:** Converts to fine-tuning ready formats\n",
        "\n",
        "We'll create multiple formats..."
      ],
      "metadata": {
        "id": "save_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Format 1: HuggingFace Dataset (Arrow format - recommended!)\n",
        "print(\"ğŸ“¦ Creating HuggingFace dataset...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  save-as data/curated/llama3_paper_qa_pairs_cleaned.json \\\n",
        "  --format ft \\\n",
        "  --storage hf\n",
        "\n",
        "# Format 2: OpenAI Fine-Tuning (JSON)\n",
        "print(\"\\nğŸ“¦ Creating OpenAI FT format...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  save-as data/curated/llama3_paper_qa_pairs_cleaned.json \\\n",
        "  --format ft\n",
        "\n",
        "# Format 3: Alpaca format\n",
        "print(\"\\nğŸ“¦ Creating Alpaca format...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  save-as data/curated/llama3_paper_qa_pairs_cleaned.json \\\n",
        "  --format alpaca\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… SAVE-AS complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show all formats\n",
        "!ls -lh data/final/"
      ],
      "metadata": {
        "id": "save_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8255f0e-181d-4ae4-bb27-f1f421d7a371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Creating HuggingFace dataset...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[?25l\u001b[32mâ ‹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ™\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¸\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ´\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¦\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ §\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "hf storage...INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ‡\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ \u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ‹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ™\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "hf storage...INFO:datasets:TensorFlow version 2.19.0 available.\n",
            "INFO:datasets:JAX version 0.7.2 available.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¸\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ´\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¦\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ §\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ‡\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ \u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ‹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ™\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¸\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2KSaving the dataset \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m/\u001b[1;36m1\u001b[0m shards\u001b[1m)\u001b[0m:   \u001b[1;36m0\u001b[0m% \u001b[1;36m0\u001b[0m/\u001b[1;36m46\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ? examples/s\u001b[1m]\u001b[0m\n",
            "\u001b[32mâ ¼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2KSaving the dataset \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m shards\u001b[1m)\u001b[0m: \u001b[1;36m100\u001b[0m% \u001b[1;36m46\u001b[0m/\u001b[1;36m46\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m6248.80\u001b[0m examples/s\u001b[1m]\u001b[0m\n",
            "\u001b[32mâ ¼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2KSaving the dataset \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m shards\u001b[1m)\u001b[0m: \u001b[1;36m100\u001b[0m% \u001b[1;36m46\u001b[0m/\u001b[1;36m46\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m5465.51\u001b[0m examples/s\u001b[1m]\u001b[0m\n",
            "\u001b[32mâ ¼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\n",
            "\u001b[32mâ ¼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "hf storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32mâœ… Converted to ft format and saved as HF dataset to \u001b[0m\n",
            "\u001b[1;32mdata/final/llama3_paper_qa_pairs_cleaned_ft_hf\u001b[0m\n",
            "\n",
            "ğŸ“¦ Creating OpenAI FT format...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[?25l\u001b[32mâ ‹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "json storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32mâœ… Converted to ft format and saved to \u001b[0m\n",
            "\u001b[1;32mdata/final/llama3_paper_qa_pairs_cleaned_ft.json\u001b[0m\n",
            "\n",
            "ğŸ“¦ Creating Alpaca format...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[?25l\u001b[32mâ ‹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to alpaca format \n",
            "with json storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32mâœ… Converted to alpaca format and saved to \u001b[0m\n",
            "\u001b[1;32mdata/final/llama3_paper_qa_pairs_cleaned_alpaca.json\u001b[0m\n",
            "\n",
            "============================================================\n",
            "âœ… SAVE-AS complete!\n",
            "============================================================\n",
            "total 36K\n",
            "-rw-r--r-- 1 root root 9.4K Oct 28 14:53 llama3_paper_qa_pairs_cleaned_alpaca.json\n",
            "drwxr-xr-x 2 root root 4.0K Oct 28 14:53 llama3_paper_qa_pairs_cleaned_ft_hf\n",
            "-rw-r--r-- 1 root root  18K Oct 28 14:53 llama3_paper_qa_pairs_cleaned_ft.json\n",
            "CPU times: user 32.2 ms, sys: 8.26 ms, total: 40.4 ms\n",
            "Wall time: 6.35 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ¯ Load & Inspect HuggingFace Dataset"
      ],
      "metadata": {
        "id": "inspect_hf_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "import json\n",
        "\n",
        "# Load the HuggingFace dataset\n",
        "dataset = load_from_disk('data/final/llama3_paper_qa_pairs_cleaned_ft_hf')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š HUGGINGFACE DATASET INFO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“¦ Dataset size: {len(dataset)} examples\")\n",
        "print(f\"\\nğŸ”§ Features: {dataset.features}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ SAMPLE TRAINING EXAMPLE (OpenAI Format)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show first example\n",
        "example = dataset[0]\n",
        "print(json.dumps(example, indent=2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Ready to use with Transformers, Axolotl, or any training framework!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "inspect_hf_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9777a8-76b8-4a42-f2fd-0ae24f988cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“Š HUGGINGFACE DATASET INFO\n",
            "============================================================\n",
            "\n",
            "ğŸ“¦ Dataset size: 46 examples\n",
            "\n",
            "ğŸ”§ Features: {'messages': List({'content': Value('string'), 'role': Value('string')})}\n",
            "\n",
            "============================================================\n",
            "ğŸ“ SAMPLE TRAINING EXAMPLE (OpenAI Format)\n",
            "============================================================\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"content\": \"You are a helpful assistant.\",\n",
            "      \"role\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"What is the size of the largest Llama 3 model in terms of parameters?\",\n",
            "      \"role\": \"user\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"405B parameters\",\n",
            "      \"role\": \"assistant\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "============================================================\n",
            "âœ… Ready to use with Transformers, Axolotl, or any training framework!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ‰ Success! Your Dataset is Ready!\n",
        "\n",
        "## ğŸ“Š Final Summary"
      ],
      "metadata": {
        "id": "summary_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Load files\n",
        "with open('data/generated/llama3_paper_qa_pairs.json', 'r') as f:\n",
        "    generated = json.load(f)\n",
        "\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json', 'r') as f:\n",
        "    curated = json.load(f)\n",
        "\n",
        "dataset = load_from_disk('data/final/llama3_paper_qa_pairs_cleaned_ft_hf')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ WORKSHOP COMPLETE - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ“š Source:\")\n",
        "print(\"   â€¢ Llama 3 Research Paper (arXiv:2407.21783)\")\n",
        "\n",
        "print(\"\\nğŸ”„ Pipeline Results:\")\n",
        "print(f\"   1ï¸âƒ£ INGEST:   âœ… PDF â†’ Clean text (.txt)\")\n",
        "print(f\"   2ï¸âƒ£ CREATE:   âœ… Generated {len(generated['qa_pairs'])} Q&A pairs (custom prompts)\")\n",
        "print(f\"   3ï¸âƒ£ CURATE:   âœ… Kept {len(curated['qa_pairs'])} high-quality pairs (â‰¥7.5/10)\")\n",
        "print(f\"   4ï¸âƒ£ SAVE-AS:  âœ… Exported to 3 formats\")\n",
        "\n",
        "metrics = curated.get('metrics', {})\n",
        "print(\"\\nğŸ“Š Quality Metrics:\")\n",
        "print(f\"   â€¢ Retention rate: {metrics.get('retention_rate', 0)*100:.1f}%\")\n",
        "print(f\"   â€¢ Average score: {metrics.get('avg_score', 0):.1f}/10\")\n",
        "\n",
        "print(\"\\nğŸ’¾ Output Formats:\")\n",
        "print(f\"   â€¢ HuggingFace Dataset: {len(dataset)} examples (Arrow format)\")\n",
        "print(f\"   â€¢ OpenAI Fine-Tuning: JSON format\")\n",
        "print(f\"   â€¢ Alpaca: JSON format\")\n",
        "\n",
        "print(\"\\nğŸ“‚ Files Location:\")\n",
        "print(\"   â€¢ data/final/ (all formats)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸš€ Your dataset is ready for fine-tuning!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ’¡ Next Steps:\")\n",
        "print(\"   â€¢ Download the dataset from data/final/\")\n",
        "print(\"   â€¢ Use with Transformers, Axolotl, or your training framework\")\n",
        "print(\"   â€¢ Fine-tune your model!\")"
      ],
      "metadata": {
        "id": "summary_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c435bd90-4340-4dcb-89c1-d83c0ed1d4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ‰ WORKSHOP COMPLETE - SUMMARY\n",
            "============================================================\n",
            "\n",
            "ğŸ“š Source:\n",
            "   â€¢ Llama 3 Research Paper (arXiv:2407.21783)\n",
            "\n",
            "ğŸ”„ Pipeline Results:\n",
            "   1ï¸âƒ£ INGEST:   âœ… PDF â†’ Clean text (.txt)\n",
            "   2ï¸âƒ£ CREATE:   âœ… Generated 50 Q&A pairs (custom prompts)\n",
            "   3ï¸âƒ£ CURATE:   âœ… Kept 46 high-quality pairs (â‰¥7.5/10)\n",
            "   4ï¸âƒ£ SAVE-AS:  âœ… Exported to 3 formats\n",
            "\n",
            "ğŸ“Š Quality Metrics:\n",
            "   â€¢ Retention rate: 92.0%\n",
            "   â€¢ Average score: 8.5/10\n",
            "\n",
            "ğŸ’¾ Output Formats:\n",
            "   â€¢ HuggingFace Dataset: 46 examples (Arrow format)\n",
            "   â€¢ OpenAI Fine-Tuning: JSON format\n",
            "   â€¢ Alpaca: JSON format\n",
            "\n",
            "ğŸ“‚ Files Location:\n",
            "   â€¢ data/final/ (all formats)\n",
            "\n",
            "============================================================\n",
            "ğŸš€ Your dataset is ready for fine-tuning!\n",
            "============================================================\n",
            "\n",
            "ğŸ’¡ Next Steps:\n",
            "   â€¢ Download the dataset from data/final/\n",
            "   â€¢ Use with Transformers, Axolotl, or your training framework\n",
            "   â€¢ Fine-tune your model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ® Bonus Experiments\n",
        "\n",
        "Try these optional experiments to explore more features!"
      ],
      "metadata": {
        "id": "experiments_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§ª Experiment 1: Try Different Quality Thresholds"
      ],
      "metadata": {
        "id": "exp1_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Strict filtering (8.5+)\n",
        "print(\"ğŸ” Testing threshold 8.5 (very strict)...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  curate data/generated/llama3_paper_qa_pairs.json \\\n",
        "  --threshold 8.5 \\\n",
        "  -o data/curated/strict_8.5.json\n",
        "\n",
        "# Lenient filtering (6.5+)\n",
        "print(\"\\nğŸ” Testing threshold 6.5 (lenient)...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  curate data/generated/llama3_paper_qa_pairs.json \\\n",
        "  --threshold 6.5 \\\n",
        "  -o data/curated/lenient_6.5.json\n",
        "\n",
        "# Compare results\n",
        "with open('data/curated/strict_8.5.json') as f:\n",
        "    strict = json.load(f)\n",
        "with open('data/curated/lenient_6.5.json') as f:\n",
        "    lenient = json.load(f)\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json') as f:\n",
        "    default = json.load(f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š THRESHOLD COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nThreshold 8.5 (strict):   {len(strict['qa_pairs'])} pairs kept\")\n",
        "print(f\"Threshold 7.5 (default):  {len(default['qa_pairs'])} pairs kept\")\n",
        "print(f\"Threshold 6.5 (lenient):  {len(lenient['qa_pairs'])} pairs kept\")\n",
        "print(\"\\nğŸ’¡ Lower threshold = more pairs, but potentially lower quality\")"
      ],
      "metadata": {
        "id": "exp1_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8cf0fb-47bb-4ffc-aeb5-9cfdce132f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Testing threshold 8.5 (very strict)...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32mğŸ”— Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KProcessing 17 batches of QA pairs...\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 50 QA pairs\n",
            "\u001b[2KRetained 29 pairs (threshold: 8.5)\n",
            "\u001b[2KAverage score: 8.5\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32mâœ… Cleaned content saved to \u001b[0m\u001b[1;32mdata/curated/strict_8.\u001b[0m\u001b[1;36m5.j\u001b[0m\u001b[1;32mson\u001b[0m\n",
            "\n",
            "ğŸ” Testing threshold 6.5 (lenient)...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32mğŸ”— Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KProcessing 17 batches of QA pairs...\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 50 QA pairs\n",
            "\u001b[2KRetained 49 pairs (threshold: 6.5)\n",
            "\u001b[2KAverage score: 8.5\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32mâœ… Cleaned content saved to \u001b[0m\u001b[1;32mdata/curated/lenient_6.\u001b[0m\u001b[1;36m5.j\u001b[0m\u001b[1;32mson\u001b[0m\n",
            "\n",
            "============================================================\n",
            "ğŸ“Š THRESHOLD COMPARISON\n",
            "============================================================\n",
            "\n",
            "Threshold 8.5 (strict):   29 pairs kept\n",
            "Threshold 7.5 (default):  46 pairs kept\n",
            "Threshold 6.5 (lenient):  49 pairs kept\n",
            "\n",
            "ğŸ’¡ Lower threshold = more pairs, but potentially lower quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§ª Experiment 2: Generate More Q&A Pairs"
      ],
      "metadata": {
        "id": "exp2_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(\"ğŸ¯ Generating 100 Q&A pairs...\\n\")\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 100 \\\n",
        "  -o data/generated/large_dataset.json \\\n",
        "  --verbose\n",
        "\n",
        "# Count pairs\n",
        "import json\n",
        "with open('data/generated/large_dataset.json') as f:\n",
        "    large = json.load(f)\n",
        "\n",
        "print(f\"\\nâœ… Generated {len(large['qa_pairs'])} Q&A pairs!\")\n",
        "print(\"\\nğŸ’¡ You can now curate this larger dataset with:\")\n",
        "print(\"   synthetic-data-kit curate data/generated/large_dataset.json\")"
      ],
      "metadata": {
        "id": "exp2_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dc143a-aab8-45ba-8580-4c7ea8b62f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Generating 100 Q&A pairs...\n",
            "\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32mğŸ”— Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KL Using api-endpoint provider\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KGenerating document summary...\n",
            "\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Sending request to api-endpoint model llama3.3-70b...\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KSummary generated (1145 chars)\n",
            "\u001b[2KGenerating QA pairs...\n",
            "\u001b[2KDocument split into 113 chunks\n",
            "\u001b[2KUsing batch size of 5\n",
            "\u001b[2KProcessing 113 chunks to generate QA pairs...\n",
            "\u001b[2KProcessing batch 1/23 with 5 chunks\n",
            "\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 132\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 1 (total: 1/100)\n",
            "\u001b[2KParsing response of length 101\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 2 (total: 2/100)\n",
            "\u001b[2KParsing response of length 117\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 3 (total: 3/100)\n",
            "\u001b[2KParsing response of length 151\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 4 (total: 4/100)\n",
            "\u001b[2KParsing response of length 411\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 5 (total: 5/100)\n",
            "\u001b[2KProcessing batch 2/23 with 5 chunks\n",
            "\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': 'Requests per minute limit exceeded - too many requests sent.', 'type': 'too_many_requests_error', 'param': 'quota', 'code': 'request_quota_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:08\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:09\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 6 (total: 6/100)\n",
            "\u001b[2KParsing response of length 156\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 7 (total: 7/100)\n",
            "\u001b[2KParsing response of length 116\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 8 (total: 8/100)\n",
            "\u001b[2KParsing response of length 122\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 9 (total: 9/100)\n",
            "\u001b[2KParsing response of length 159\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 10 (total: 10/100)\n",
            "\u001b[2KProcessing batch 3/23 with 5 chunks\n",
            "\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:09\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:09\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:09\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:10\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:11\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 113\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 11 (total: 11/100)\n",
            "\u001b[2KParsing response of length 167\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 12 (total: 12/100)\n",
            "\u001b[2KParsing response of length 156\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 13 (total: 13/100)\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 14 (total: 14/100)\n",
            "\u001b[2KParsing response of length 181\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 15 (total: 15/100)\n",
            "\u001b[2KProcessing batch 4/23 with 5 chunks\n",
            "\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:12\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:12\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:13\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:13\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:14\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 212\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 16 (total: 16/100)\n",
            "\u001b[2KParsing response of length 123\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 17 (total: 17/100)\n",
            "\u001b[2KParsing response of length 172\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 18 (total: 18/100)\n",
            "\u001b[2KParsing response of length 131\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 19 (total: 19/100)\n",
            "\u001b[2KParsing response of length 129\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 20 (total: 20/100)\n",
            "\u001b[2KProcessing batch 5/23 with 5 chunks\n",
            "\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:15\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:15\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:16\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:17\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 122\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 21 (total: 21/100)\n",
            "\u001b[2KParsing response of length 207\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 22 (total: 22/100)\n",
            "\u001b[2KParsing response of length 202\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 23 (total: 23/100)\n",
            "\u001b[2KParsing response of length 169\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 24 (total: 24/100)\n",
            "\u001b[2KParsing response of length 224\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 25 (total: 25/100)\n",
            "\u001b[2KProcessing batch 6/23 with 5 chunks\n",
            "\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:18\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:19\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ™\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:19\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 277\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 26 (total: 26/100)\n",
            "\u001b[2KParsing response of length 144\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 27 (total: 27/100)\n",
            "\u001b[2KParsing response of length 203\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 28 (total: 28/100)\n",
            "\u001b[2KParsing response of length 254\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 29 (total: 29/100)\n",
            "\u001b[2KParsing response of length 236\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 30 (total: 30/100)\n",
            "\u001b[2KProcessing batch 7/23 with 5 chunks\n",
            "\u001b[32mâ ¹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ §\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:21\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ¦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:22\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:22\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ ´\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32mâ \u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 201\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 31 (total: 31/100)\n",
            "\u001b[2KParsing response of length 283\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 32 (total: 32/100)\n",
            "\u001b[2KParsing response of length 208\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 33 (total: 33/100)\n",
            "\u001b[2KParsing response of length 323\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 34 (total: 34/100)\n",
            "\u001b[2KParsing response of length 238\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 35 (total: 35/100)\n",
            "\u001b[2KProcessing batch 8/23 with 5 chunks\n",
            "\u001b[32mâ ‹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32mâ ¼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:24\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:24\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:25\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:26\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:synthetic_data_kit.models.llm_client:api-endpoint API error (attempt 1/3): Error code: 429 - {'message': 'Requests per minute limit exceeded - too many requests sent.', 'type': 'too_many_requests_error', 'param': 'quota', 'code': 'request_quota_exceeded'}\n",
            "\u001b[2KGenerating QA pairs \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:01:27\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 60.000000 seconds\n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Generating qa content from data/parsed/llama3_paper.txt..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§ª Experiment 3: Different Chunking Strategies"
      ],
      "metadata": {
        "id": "exp3_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Small chunks (more granular)\n",
        "print(\"ğŸ“ Testing small chunks (2000 chars)...\\n\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 20 \\\n",
        "  --chunk-size 2000 \\\n",
        "  --chunk-overlap 100 \\\n",
        "  -o data/generated/small_chunks.json\n",
        "\n",
        "# Large chunks (more context)\n",
        "print(\"\\nğŸ“ Testing large chunks (6000 chars)...\\n\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 20 \\\n",
        "  --chunk-size 6000 \\\n",
        "  --chunk-overlap 300 \\\n",
        "  -o data/generated/large_chunks.json\n",
        "\n",
        "# Compare questions\n",
        "with open('data/generated/small_chunks.json') as f:\n",
        "    small = json.load(f)\n",
        "with open('data/generated/large_chunks.json') as f:\n",
        "    large = json.load(f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š CHUNKING COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ”¬ Small Chunks (2000 chars) - Sample Question:\")\n",
        "print(f\"   {small['qa_pairs'][0]['question']}\")\n",
        "\n",
        "print(\"\\nğŸ“š Large Chunks (6000 chars) - Sample Question:\")\n",
        "print(f\"   {large['qa_pairs'][0]['question']}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Small chunks = more specific questions\")\n",
        "print(\"ğŸ’¡ Large chunks = more context-aware questions\")"
      ],
      "metadata": {
        "id": "exp3_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  Experiment 4: Chain-of-Thought Enhancement\n",
        "\n",
        "**Advanced:** Add reasoning steps to your Q&A pairs using custom CoT prompts!"
      ],
      "metadata": {
        "id": "exp4_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create CoT config with custom enhancement prompt\n",
        "cot_config = f\"\"\"llm:\n",
        "  provider: \"api-endpoint\"\n",
        "\n",
        "api-endpoint:\n",
        "  api_base: \"https://api.cerebras.ai/v1\"\n",
        "  api_key: \"{os.environ.get('CEREBRAS_API_KEY')}\"\n",
        "  model: \"llama3.3-70b\"\n",
        "\n",
        "generation:\n",
        "  temperature: 0.2\n",
        "  max_tokens: 8192\n",
        "\n",
        "prompts:\n",
        "  cot_enhancement: |\n",
        "    You are enhancing Q&A conversations by adding step-by-step reasoning.\n",
        "\n",
        "    For each assistant response, add detailed reasoning BEFORE the answer:\n",
        "\n",
        "    Transform:\n",
        "    Q: \"What is Llama 3's context length?\"\n",
        "    A: \"128K tokens\"\n",
        "\n",
        "    Into:\n",
        "    Q: \"What is Llama 3's context length?\"\n",
        "    A: \"Let me break this down:\n",
        "    Step 1: Looking at the architecture section...\n",
        "    Step 2: The paper states...\n",
        "    Therefore: Llama 3 supports 128K tokens\"\n",
        "\n",
        "    Enhance these conversations:\n",
        "    {{{{conversations}}}}\n",
        "\"\"\"\n",
        "\n",
        "with open('cot_config.yaml', 'w') as f:\n",
        "    f.write(cot_config)\n",
        "\n",
        "print(\"âœ… CoT config created with custom enhancement prompt!\\n\")\n",
        "\n",
        "# Step 2: Generate simple Q&A\n",
        "print(\"ğŸ“ Generating 10 simple Q&A pairs...\\n\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 10 \\\n",
        "  -o data/generated/simple_for_cot.json\n",
        "\n",
        "# Step 3: Add reasoning\n",
        "print(\"\\nğŸ§  Adding Chain-of-Thought reasoning...\\n\")\n",
        "!synthetic-data-kit -c cot_config.yaml \\\n",
        "  create data/generated/simple_for_cot.json \\\n",
        "  --type cot-enhance \\\n",
        "  -o data/generated/with_reasoning.json \\\n",
        "  --verbose\n",
        "\n",
        "print(\"\\nâœ… Chain-of-Thought enhancement complete!\")"
      ],
      "metadata": {
        "id": "exp4_cell_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Compare before and after\n",
        "with open('data/generated/simple_for_cot.json') as f:\n",
        "    before = json.load(f)\n",
        "with open('data/generated/with_reasoning.json') as f:\n",
        "    after = json.load(f)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ” CHAIN-OF-THOUGHT COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get first Q&A from conversations\n",
        "before_conv = before['qa_pairs'][0]\n",
        "after_conv = after[0]['conversations'] if isinstance(after, list) else after['conversations'][0]\n",
        "\n",
        "print(\"\\nğŸ“ BEFORE (Simple answer):\")\n",
        "print(f\"Q: {before_conv['question']}\")\n",
        "print(f\"A: {before_conv['answer'][:150]}...\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "print(\"\\nğŸ§  AFTER (With reasoning):\")\n",
        "for msg in after_conv:\n",
        "    if msg['role'] == 'user':\n",
        "        print(f\"Q: {msg['content']}\")\n",
        "    elif msg['role'] == 'assistant':\n",
        "        print(f\"A: {msg['content'][:300]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ¨ Notice the step-by-step reasoning in the enhanced version!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "exp4_cell_compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ“¥ Download Your Dataset\n",
        "\n",
        "Download the files to your local machine:"
      ],
      "metadata": {
        "id": "download_files_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a zip file with all outputs\n",
        "!zip -r llama3_dataset.zip data/final/\n",
        "\n",
        "print(\"âœ… Dataset packaged!\")\n",
        "print(\"\\nğŸ“¦ Download 'llama3_dataset.zip' from the Files panel (left sidebar)\")\n",
        "print(\"   Or run this cell and click the download link below:\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('llama3_dataset.zip')"
      ],
      "metadata": {
        "id": "download_files_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ“ Workshop Complete!\n",
        "\n",
        "## What You Accomplished:\n",
        "\n",
        "âœ… **Parsed** a research paper automatically (to .txt format)  \n",
        "âœ… **Generated** 50+ Q&A pairs using Cerebras with custom prompts  \n",
        "âœ… **Filtered** for quality using LLM-as-judge with custom rating criteria  \n",
        "âœ… **Exported** to multiple training formats  \n",
        "âœ… **Learned** advanced features (CoT, chunking, thresholds, custom prompts)  \n",
        "\n",
        "## ğŸš€ Next Steps:\n",
        "\n",
        "1. **Try your own PDFs** - Upload any research paper or document\n",
        "2. **Customize prompts** - Edit the prompts in the config for your domain\n",
        "3. **Adjust parameters** - Experiment with thresholds, chunk sizes, etc.\n",
        "4. **Fine-tune a model** - Use your dataset with Transformers/Axolotl\n",
        "5. **Scale up** - Process entire directories of documents\n",
        "\n",
        "## ğŸ“š Resources:\n",
        "\n",
        "- **Toolkit:** https://github.com/meta-llama/synthetic-data-kit\n",
        "- **Cerebras API:** https://cerebras.ai/\n",
        "- **Documentation:** Check the toolkit README for advanced features\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ Happy Dataset Building!**"
      ],
      "metadata": {
        "id": "conclusion_section"
      }
    }
  ]
}